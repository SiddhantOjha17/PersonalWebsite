[
  {
    "slug": "how-ldms-learn-to-reason",
    "title": "How Language Diffusion Models Learn to Reason",
    "excerpt": "An exploration of how Reinforcement Learning (RL) teaches diffusion models to reason, overcoming their non-sequential nature with methods like diffu-GRPO, DCoLT, and SEPO.",
    "content_format": "markdown",
    "published_at": "27-06-2025",
    "tags": ["Language Diffusion Models", "Reinforcement Learning", "Reasoning"],
    "hero_image": "/assets/HowLDMReason.png",
    "medium_link": "https://medium.com/@siddhantojha17/how-language-diffusion-models-learn-to-reason-6fb02f1e52f7",
    "content": "# How Do We Teach Reasoning to Diffusion Models?\n\nWe’ve seen that diffusion models are flexible and powerful, but can they reason? Sure, they can fill in blanks and complete poems backwards but reasoning is about multi-step logic. Like solving math problems, planning moves in a game, or writing code with clear intent. That kind of ability often requires training the model to think, not just speak.\n\nJust like we taught GPTs to be helpful and safe using Reinforcement Learning with Human Feedback (RLHF), we can use Reinforcement Learning (RL) to teach diffusion models how to reason.\n\n## What Is Reinforcement-Based Reasoning?\n\nAt its heart, reinforcement learning (RL) is a way of teaching models to think by trial and error, rather than by rote memorization. Instead of training a model to simply reproduce what it has seen before, RL teaches it to explore options, make decisions, and learn from consequences.\n\nThink of it like this: Instead of saying “Here’s the correct answer, memorize it,” we say, “If you reach the correct answer however you get there you’ll earn a reward. Now figure out the best way on your own.”\n\nThis learning-by-reward mechanism is what enabled AlphaGo to master the ancient game of Go. It’s how models like GPT-4 learned to follow human preferences through Reinforcement Learning from Human Feedback (RLHF). And now, this same idea is being applied to help diffusion-based language models not just generate text, but reason through problems and arrive at solutions.\n\n## The Unique Challenge with LDMs\n\nHere’s the twist: applying reinforcement learning to GPT-style models is already well-established. They generate one token at a time, in a strict left-to-right order. But diffusion language models play by an entirely different rulebook. They don’t generate in a fixed order. They might predict multiple masked tokens simultaneously, fill in a sentence from the middle, or denoise tokens randomly. This creates a major challenge: how do you assign credit when many parts of the output are generated together?\n\nThis is where recent research like diffu-GRPO, DCoLT, and SEPO is making breakthroughs.\n\n## 1. diffu-GRPO: Gradient-Based Reinforcement for Masked Diffusion\n\n**diffu-GRPO (Gradient-Based Reinforcement Policy Optimization)** is a new RL method designed for masked diffusion LMs like LLaDA. Since the model predicts multiple masked tokens at once, we don’t know how to assign credit to individual predictions or compute gradients using sequence-level rewards.\n\n**The Solution:** diffu-GRPO estimates the log probability of a sequence by averaging over possible masking patterns (a mean-field approximation). It then computes the policy gradient to make high-reward outputs more likely.\n\n## 2. DCoLT: Treating Diffusion as a Reasoning Game\n\n**DCoLT (Diffusion Chain of Lateral Thought)** turns the diffusion process itself into a reasoning policy. Each step, like choosing which word to unmask next, is treated as an action in a turn-based game. The goal is to reach the final correct answer in as few moves as possible. This turns the denoising process into a sequential decision-making problem, which is exactly what RL is built for.\n\n## 3. SEPO: Reinforcement for Discrete Diffusion\n\n**SEPO (Score Entropy Policy Optimization)** is designed for discrete diffusion models. Since the model’s outputs are categorical and rewards are non-differentiable, SEPO builds on policy gradient theory to estimate how much each token choice contributes to the final reward. It then updates the model to increase the probability of high-reward token choices.\n\n## Conclusion: Reasoning Beyond the Left-to-Right Mindset\n\nBy moving beyond rigid left-to-right generation, LDMs open the door to more flexible, bidirectional, and creative thinking. With reinforcement learning, we can guide these models to reason, solve, and reflect, not just autocomplete. We’re training models to search, explore, evaluate, and correct themselves—much like how humans think."
  },
  {
    "slug": "language-diffusion-models-overview",
    "title": "Language Diffusion Models: A Brief Overview",
    "excerpt": "An introduction to Language Diffusion Models (LDMs) as a non-autoregressive alternative to traditional LLMs, exploring masked and continuous diffusion methods.",
    "content_format": "markdown",
    "published_at": "27-06-2025",
    "tags": ["Language Diffusion Models", "Generative AI", "NLP"],
    "hero_image": "assets/LDMOverview.png",
    "medium_link": "https://medium.com/@siddhantojha17/language-diffusion-models-a-brief-overview-3eb3d1f75bb7",
    "content": "# Language Diffusion Models: A Brief Overview\n\nLarge Language Models (LLMs) like GPT and PaLM have dominated NLP. These autoregressive models have impressive capabilities but come with limitations in reasoning and controllability. Enter **Language Diffusion Models (LDMs)**, a new class of generative models that reimagine text generation as an iterative denoising process.\n\n## How are they different from LLMs?\n\nTraditional LLMs are autoregressive, predicting the next token based on previous context. LDMs, inspired by image generation models like DALL·E 2, are different. They start with corrupted data (noise) and train a model to reverse the corruption step-by-step.\n\n## Two Types of Diffusion Models\n\n1.  **Masked Diffusion (Discrete)**: Instead of adding mathematical noise, we mask random words in a sentence. The model is trained to predict the masked tokens, gradually turning a mostly masked sentence into the original. The model LLaDA uses this approach.\n2.  **Continuous Diffusion**: Here, we work in the embedding space. We add continuous Gaussian noise to the word vectors and train a model to denoise them back to their original state. The Plaid model is an example of this method.\n\n## Diffusion vs. Autoregressive Models: What’s the Difference?\n\n- **Generation Direction**: Autoregressive models generate left-to-right. Diffusion models are order-agnostic, allowing them to fill in text from the middle, from both ends, or handle other flexible editing tasks.\n- **Bidirectional Context**: Diffusion models can see context both before and after a masked word, making them better at nuanced predictions.\n- **Flexible Editing**: LDMs excel at tasks like infilling, text rewriting, and even reversal tasks like writing poems backward, where models like LLaDA have outperformed GPT-4o.\n\n## Conclusion\n\nLDMs offer a compelling new paradigm for text generation. Their ability to generate text non-autoregressively, leverage bidirectional context, and excel in flexible editing tasks marks a significant leap forward. They promise a future where text generation is more controllable, diverse, and adept at understanding the holistic context of language."
  }
]